# Neural Networks

- Lab 7 is well-annotated and ideal for studying.
- Asg 6 was created after lab 7 and is the dumbed down version of it

## Questions

- [ ] How is the derivative of the 'loss' (or equivalent) w.r.t AL (for any layer prior to the final layer) equal to `delta2 @ self.W[L+1].T`
- [ ] Why does MSE + Sigmoid suck?
- [ ] How does the derivative of BCE + Sigmoid applied on a layer during back-prop simplify to such an elegant expression?
